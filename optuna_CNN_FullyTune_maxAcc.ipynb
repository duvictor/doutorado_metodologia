{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "587U3hHFYjnA"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchsummary'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_35356\\3799572652.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtorchvision\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtorchvision\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtransforms\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtorchsummary\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0msummary\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     12\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[0mget_ipython\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msystem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'pip install optuna'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'torchsummary'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "!pip install optuna\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IO7PB0saYwVq"
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\")  ##'cuda' or 'cpu'\n",
    "BATCHSIZE = 128\n",
    "CLASSES = 10   #CLASSES = 10 for cifar10 and 100 for cifar100\n",
    "DIR = os.getcwd()\n",
    "EPOCHS = 10\n",
    "LOG_INTERVAL = 10\n",
    "N_TRAIN_EXAMPLES = BATCHSIZE * 30\n",
    "N_VALID_EXAMPLES = BATCHSIZE * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zx8QaqLY1NK"
   },
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    output_channels1 = trial.suggest_int(name=\"filters_1\", low=32, high=64, step=32)\n",
    "    layers.append(nn.Conv2d(in_channels=3, out_channels=output_channels1, kernel_size=3, stride=1))\n",
    "    layers.append(nn.BatchNorm2d(output_channels1))\n",
    "    layers.append(nn.ReLU())\n",
    "    p1 = trial.suggest_float(name=\"dropout_l\", low=0.2, high=0.4)\n",
    "    layers.append(nn.Dropout(p1))\n",
    "\n",
    "    output_channels2 = trial.suggest_int(name=\"filters_2\", low=64, high=128, step=32)\n",
    "    layers.append(nn.Conv2d(in_channels=output_channels1, out_channels=output_channels2, kernel_size=3, stride=2))\n",
    "    layers.append(nn.BatchNorm2d(output_channels2))\n",
    "    layers.append(nn.ReLU())\n",
    "    p2 = trial.suggest_float(name=\"dropout_2\", low=0.2, high=0.4)\n",
    "    layers.append(nn.Dropout(p2))\n",
    "\n",
    "    layers.append(nn.Conv2d(in_channels=output_channels2, out_channels=128, kernel_size=3, stride=2))\n",
    "    layers.append(nn.BatchNorm2d(128))\n",
    "    layers.append(nn.ReLU())\n",
    "    layers.append(nn.Dropout(0.2))\n",
    "\n",
    "    layers.append(nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2))\n",
    "    layers.append(nn.BatchNorm2d(256))\n",
    "    layers.append(nn.ReLU())\n",
    "    \n",
    "    layers.append(nn.Flatten())\n",
    "    output_units1 = trial.suggest_int(name=\"linear_1\", low=128, high=512, step=128)\n",
    "    layers.append(nn.Linear(256*2*2, output_units1))  #output size found by printing the model detail using summary in torchsummary \n",
    "    layers.append(nn.Dropout(0.2))\n",
    "    layers.append(nn.Linear(output_units1, CLASSES))  #CLASSES = 10 for cifar10 and 100 for cifar100\n",
    "    #cross entropy loss used as loss function, therefore no softmax layer here\n",
    "\n",
    "    return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4apuxgZuHbB"
   },
   "outputs": [],
   "source": [
    "def get_cifar10():\n",
    "    # Load cifar10 dataset.\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root=DIR, train=True,\n",
    "                                        download=True, transform=transform)\n",
    "    \n",
    "    #split training data into training-80% and validation-20%\n",
    "    train_set, val_set = torch.utils.data.random_split(trainset, [int(0.8*len(trainset)), int(0.2*len(trainset))])\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCHSIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "    \n",
    "    valid_loader = torch.utils.data.DataLoader(val_set, batch_size=BATCHSIZE,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    testset = torchvision.datasets.CIFAR10(root=DIR, train=False,\n",
    "                                       download=True, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=BATCHSIZE,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "    \"\"\"\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vLM6cAdcvcod"
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # Generate the model.\n",
    "    model = define_model(trial).to(DEVICE)\n",
    "\n",
    "    # Generate the optimizers.\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"]) #for hp tuning\n",
    "    #optimizer_name = \"Adam\"\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True) #for hp tuning\n",
    "    #lr = 0.001\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "    CEloss = nn.CrossEntropyLoss()  ## this loss object must be used the loop. Directly using nn.CrossEntropyLoss() gives error\n",
    "\n",
    "    # Get the MNIST dataset.\n",
    "    train_loader, valid_loader = get_cifar10()\n",
    "\n",
    "    # Training of the model.\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # Limiting training data for faster epochs.\n",
    "            if batch_idx * BATCHSIZE >= N_TRAIN_EXAMPLES:\n",
    "                break\n",
    "\n",
    "            #data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)  ## for mnist\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)  ## for cifar 10 and 100\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = CEloss(output, target)  ## used cross entropy loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation of the model.\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            val_loss_batch = 0\n",
    "            for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "                # Limiting validation data.\n",
    "                if batch_idx * BATCHSIZE >= N_VALID_EXAMPLES:\n",
    "                    break\n",
    "                #data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)  ## for mnist\n",
    "                data, target = data.to(DEVICE), target.to(DEVICE)  ## for cifar 10 and 100\n",
    "                output = model(data)\n",
    "                # Get the index of the max log-probability.\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "                val_loss_batch += CEloss(output, target).item()  ## used cross entropy loss\n",
    "\n",
    "        accuracy = correct / min(len(valid_loader.dataset), N_VALID_EXAMPLES)\n",
    "        #val_loss_epoch = val_loss_batch / min(len(valid_loader.dataset), N_VALID_EXAMPLES)\n",
    "\n",
    "        trial.report(accuracy, epoch)\n",
    "        #trial.report(val_loss_epoch, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return accuracy #val_loss_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00dcwGRRweud"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")  # 'maximize' because objective function is returning accuracy\n",
    "    #study = optuna.create_study(direction=\"minimize\")  # 'minimize' because objective function is returning loss\n",
    "    study.optimize(objective, n_trials=30, timeout=600)\n",
    "\n",
    "    pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "    complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_YAn0L5k5i-F"
   },
   "outputs": [],
   "source": [
    "study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBBRqyRr5mU2"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0sDjm3U5ooR"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study) ## this is important to figure out which hp is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cL7VIquW5xN-"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study)   ## this gives a clear picture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WpXM9lSe6GBg"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQss4aRjfUme"
   },
   "outputs": [],
   "source": [
    "# SKIP THIS\n",
    "#### used for testing output sizes of layers in the model\n",
    "#****important: only change the input filter to maintain the output size of each layer\n",
    "\"\"\"\n",
    "model = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1)\n",
    "    ,nn.BatchNorm2d(32)\n",
    "    ,nn.ReLU()\n",
    "    ,nn.Dropout(0.2)\n",
    "    ,nn.Conv2d(in_channels=32, out_channels=128, kernel_size=3, stride=2)\n",
    "    ,nn.BatchNorm2d(128) #this must be same as the out_channel of the previous layer\n",
    "    ,nn.ReLU()\n",
    "    ,nn.Dropout(0.2)\n",
    "    ,nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2)\n",
    "    ,nn.BatchNorm2d(128)\n",
    "    ,nn.ReLU()\n",
    "    ,nn.Dropout(0.2)\n",
    "    ,nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2)\n",
    "    ,nn.BatchNorm2d(256)\n",
    "    ,nn.ReLU()\n",
    "    ,nn.Flatten()\n",
    "    ,nn.Linear(256*2*2, 500)  #output size found by printing the model detail using summary in torchsummary \n",
    "    ,nn.Dropout(0.2)\n",
    "    ,nn.Linear(500, CLASSES))  #CLASSES = 10 for cifar10 and 100 for cifar100\n",
    "\n",
    "print(summary(model,(3,32,32)))\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP8DvsG676R67IYJVm7I1sy",
   "collapsed_sections": [],
   "name": "optuna_CNN_FullyTune_minLoss.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}