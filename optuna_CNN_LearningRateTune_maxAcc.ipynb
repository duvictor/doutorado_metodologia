{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"optuna_CNN_LearningRateTune_maxAcc.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM+Y2VYa64/npcy1I7AqggQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"587U3hHFYjnA"},"source":["import os\r\n","\r\n","import torch\r\n","import torch.nn as nn\r\n","import torch.nn.functional as F\r\n","import torch.optim as optim\r\n","import torch.utils.data\r\n","import torchvision\r\n","from torchvision import datasets\r\n","from torchvision import transforms\r\n","from torchsummary import summary\r\n","\r\n","!pip install optuna\r\n","import optuna"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IO7PB0saYwVq"},"source":["DEVICE = torch.device(\"cuda\")  ##'cuda' or 'cpu'\r\n","BATCHSIZE = 128\r\n","CLASSES = 10   #CLASSES = 10 for cifar10 and 100 for cifar100\r\n","DIR = os.getcwd()\r\n","EPOCHS = 10\r\n","LOG_INTERVAL = 10\r\n","N_TRAIN_EXAMPLES = BATCHSIZE * 30\r\n","N_VALID_EXAMPLES = BATCHSIZE * 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_zx8QaqLY1NK"},"source":["def define_model(trial):\r\n","\r\n","    layers = []\r\n","\r\n","    layers.append(nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1))\r\n","    layers.append(nn.BatchNorm2d(32))\r\n","    layers.append(nn.ReLU())\r\n","    layers.append(nn.Dropout(0.2))\r\n","\r\n","    layers.append(nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2))\r\n","    layers.append(nn.BatchNorm2d(64))\r\n","    layers.append(nn.ReLU())\r\n","    layers.append(nn.Dropout(0.2))\r\n","\r\n","    layers.append(nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2))\r\n","    layers.append(nn.BatchNorm2d(128))\r\n","    layers.append(nn.ReLU())\r\n","    layers.append(nn.Dropout(0.2))\r\n","\r\n","    layers.append(nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2))\r\n","    layers.append(nn.BatchNorm2d(256))\r\n","    layers.append(nn.ReLU())\r\n","    \r\n","    layers.append(nn.Flatten())\r\n","    layers.append(nn.Linear(256*2*2, 500))  #output size found by printing the model detail using summary in torchsummary \r\n","    layers.append(nn.Dropout(0.2))\r\n","    layers.append(nn.Linear(500, CLASSES))  #CLASSES = 10 for cifar10 and 100 for cifar100\r\n","    #cross entropy loss used as loss function, therefore no softmax layer here\r\n","\r\n","    return nn.Sequential(*layers)\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n4apuxgZuHbB"},"source":["def get_cifar10():\r\n","    # Load cifar10 dataset.\r\n","\r\n","    transform = transforms.Compose(\r\n","    [transforms.ToTensor(),\r\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\r\n","\r\n","    trainset = torchvision.datasets.CIFAR10(root=DIR, train=True,\r\n","                                        download=True, transform=transform)\r\n","    \r\n","    #split training data into training-80% and validation-20%\r\n","    train_set, val_set = torch.utils.data.random_split(trainset, [int(0.8*len(trainset)), int(0.2*len(trainset))])\r\n","\r\n","    train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCHSIZE,\r\n","                                          shuffle=True, num_workers=2)\r\n","    \r\n","    valid_loader = torch.utils.data.DataLoader(val_set, batch_size=BATCHSIZE,\r\n","                                         shuffle=False, num_workers=2)\r\n","    \r\n","\r\n","    \"\"\"\r\n","    testset = torchvision.datasets.CIFAR10(root=DIR, train=False,\r\n","                                       download=True, transform=transform)\r\n","    test_loader = torch.utils.data.DataLoader(testset, batch_size=BATCHSIZE,\r\n","                                         shuffle=False, num_workers=2)\r\n","    \"\"\"\r\n","\r\n","    return train_loader, valid_loader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vLM6cAdcvcod"},"source":["def objective(trial):\r\n","\r\n","    # Generate the model.\r\n","    model = define_model(trial).to(DEVICE)\r\n","\r\n","    # Generate the optimizers.\r\n","    #optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"]) #for hp tuning\r\n","    optimizer_name = \"Adam\"\r\n","    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True) #for hp tuning\r\n","    #lr = 0.001\r\n","    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\r\n","    CEloss = nn.CrossEntropyLoss()  ## this loss object must be used the loop. Directly using nn.CrossEntropyLoss() gives error\r\n","\r\n","    # Get the MNIST dataset.\r\n","    train_loader, valid_loader = get_cifar10()\r\n","\r\n","    # Training of the model.\r\n","    for epoch in range(EPOCHS):\r\n","        model.train()\r\n","        for batch_idx, (data, target) in enumerate(train_loader):\r\n","            # Limiting training data for faster epochs.\r\n","            if batch_idx * BATCHSIZE >= N_TRAIN_EXAMPLES:\r\n","                break\r\n","\r\n","            #data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)  ## for mnist\r\n","            data, target = data.to(DEVICE), target.to(DEVICE)  ## for cifar 10 and 100\r\n","\r\n","            optimizer.zero_grad()\r\n","            output = model(data)\r\n","            loss = CEloss(output, target)  ## used cross entropy loss\r\n","            loss.backward()\r\n","            optimizer.step()\r\n","\r\n","        # Validation of the model.\r\n","        model.eval()\r\n","        correct = 0\r\n","        with torch.no_grad():\r\n","            val_loss_batch = 0\r\n","            for batch_idx, (data, target) in enumerate(valid_loader):\r\n","                # Limiting validation data.\r\n","                if batch_idx * BATCHSIZE >= N_VALID_EXAMPLES:\r\n","                    break\r\n","                #data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)  ## for mnist\r\n","                data, target = data.to(DEVICE), target.to(DEVICE)  ## for cifar 10 and 100\r\n","                output = model(data)\r\n","                # Get the index of the max log-probability.\r\n","                pred = output.argmax(dim=1, keepdim=True)\r\n","                correct += pred.eq(target.view_as(pred)).sum().item()\r\n","\r\n","                val_loss_batch += CEloss(output, target).item()  ## used cross entropy loss\r\n","\r\n","        accuracy = correct / min(len(valid_loader.dataset), N_VALID_EXAMPLES)\r\n","        #val_loss_epoch = val_loss_batch / min(len(valid_loader.dataset), N_VALID_EXAMPLES)\r\n","\r\n","        trial.report(accuracy, epoch)\r\n","        #trial.report(val_loss_epoch, epoch)\r\n","\r\n","        # Handle pruning based on the intermediate value.\r\n","        if trial.should_prune():\r\n","            raise optuna.exceptions.TrialPruned()\r\n","\r\n","    return accuracy #val_loss_epoch "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"00dcwGRRweud"},"source":["if __name__ == \"__main__\":\r\n","    study = optuna.create_study(direction=\"maximize\")  # 'maximize' because objective function is returning accuracy\r\n","    study.optimize(objective, n_trials=50, timeout=600) \r\n","\r\n","    pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\r\n","    complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\r\n","\r\n","    print(\"Study statistics: \")\r\n","    print(\"  Number of finished trials: \", len(study.trials))\r\n","    print(\"  Number of pruned trials: \", len(pruned_trials))\r\n","    print(\"  Number of complete trials: \", len(complete_trials))\r\n","\r\n","    print(\"Best trial:\")\r\n","    trial = study.best_trial\r\n","\r\n","    print(\"  Value: \", trial.value)\r\n","\r\n","    print(\"  Params: \")\r\n","    for key, value in trial.params.items():\r\n","        print(\"    {}: {}\".format(key, value))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_YAn0L5k5i-F"},"source":["study.best_trial"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HBBRqyRr5mU2"},"source":["optuna.visualization.plot_optimization_history(study)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z0sDjm3U5ooR"},"source":["optuna.visualization.plot_param_importances(study) ## this is important to figure out which hp is important"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cL7VIquW5xN-"},"source":["optuna.visualization.plot_slice(study)   ## this gives a clear picture "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WpXM9lSe6GBg"},"source":["optuna.visualization.plot_parallel_coordinate(study)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQss4aRjfUme"},"source":["# SKIP THIS\r\n","#### used for testing output sizes of layers in the model\r\n","#****important: only change the input filter to maintain the output size of each layer\r\n","\"\"\"\r\n","model = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1)\r\n","    ,nn.BatchNorm2d(32)\r\n","    ,nn.ReLU()\r\n","    ,nn.Dropout(0.2)\r\n","    ,nn.Conv2d(in_channels=32, out_channels=128, kernel_size=3, stride=2)\r\n","    ,nn.BatchNorm2d(128) #this must be same as the out_channel of the previous layer\r\n","    ,nn.ReLU()\r\n","    ,nn.Dropout(0.2)\r\n","    ,nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2)\r\n","    ,nn.BatchNorm2d(128)\r\n","    ,nn.ReLU()\r\n","    ,nn.Dropout(0.2)\r\n","    ,nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2)\r\n","    ,nn.BatchNorm2d(256)\r\n","    ,nn.ReLU()\r\n","    ,nn.Flatten()\r\n","    ,nn.Linear(256*2*2, 500)  #output size found by printing the model detail using summary in torchsummary \r\n","    ,nn.Dropout(0.2)\r\n","    ,nn.Linear(500, CLASSES))  #CLASSES = 10 for cifar10 and 100 for cifar100\r\n","\r\n","print(summary(model,(3,32,32)))\r\n","\"\"\""],"execution_count":null,"outputs":[]}]}